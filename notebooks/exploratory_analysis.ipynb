{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddda75dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d82c5",
   "metadata": {},
   "source": [
    "## 1. Load and Augmenting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b502d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Found cached preprocessed data - loading from cache...\n",
      "\n",
      "============================================================\n",
      "LOADING PREPROCESSED DATA FROM CACHE\n",
      "============================================================\n",
      "Loading from: ..\\data\\processed\\preprocessed_uniform.npz\n",
      "âœ“ Loaded in 1.19 seconds\n",
      "âœ“ Images shape: (2421, 224, 224, 3)\n",
      "âœ“ Labels shape: (2421,)\n",
      "âœ“ Unique classes: [0 1 2 3 4 5]\n",
      "============================================================\n",
      "\n",
      "âœ“ Using cached data - MUCH FASTER than loading from folders!\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS (UNIFORM 30%):\n",
      "============================================================\n",
      "Dataset shape: (2421, 224, 224, 3)\n",
      "Labels shape: (2421,)\n",
      "Total images: 2421\n",
      "\n",
      "ðŸ“Š Uniform class distribution:\n",
      "  glass: 500 images (20.7%)\n",
      "  paper: 583 images (24.1%)\n",
      "  cardboard: 321 images (13.3%)\n",
      "  plastic: 471 images (19.5%)\n",
      "  metal: 409 images (16.9%)\n",
      "  trash: 137 images (5.7%)\n",
      "\n",
      "âœ… Uniform augmentation (30% per class):\n",
      "   â€¢ Each class increased by 30%\n",
      "   â€¢ Maintains original distribution\n",
      "   â€¢ No risk of overfitting to augmented minority samples\n",
      "\n",
      "============================================================\n",
      "âœ“ Data ready for feature extraction!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"../dataset\"\n",
    "OUTPUT_PATH = \"../data/processed\"\n",
    "CACHE_FILE = \"../data/processed/preprocessed_uniform.npz\"  # Uniform 30% augmentation\n",
    "\n",
    "from pathlib import Path\n",
    "from data_preprocessing import (\n",
    "    load_dataset, \n",
    "    augment_dataset, \n",
    "    save_preprocessed_data,\n",
    "    load_preprocessed_data\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Check if cached preprocessed data exists\n",
    "    cache_path = Path(CACHE_FILE)\n",
    "    \n",
    "    if cache_path.exists():\n",
    "        print(\"ðŸ“¦ Found cached preprocessed data - loading from cache...\")\n",
    "        augmented_images, augmented_labels = load_preprocessed_data(CACHE_FILE)\n",
    "        \n",
    "        print(\"\\nâœ“ Using cached data - MUCH FASTER than loading from folders!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"ðŸ”„ No cache found - loading from folders and preprocessing...\")\n",
    "        print(\"ðŸ’¡ UNIFORM AUGMENTATION: 30% increase per class\")\n",
    "        \n",
    "        # Load original dataset\n",
    "        images, labels, class_names = load_dataset(DATASET_PATH)\n",
    "        \n",
    "        # Augment dataset with UNIFORM distribution\n",
    "        # 30% increase per class (uniform)\n",
    "        augmented_images, augmented_labels = augment_dataset(\n",
    "            images, labels, \n",
    "            augmentation_factor=0.3,  # 30% per class\n",
    "            balance_classes=False     # Uniform distribution\n",
    "        )\n",
    "        \n",
    "        # Save to cache for next time\n",
    "        print(\"\\nðŸ’¾ Saving preprocessed data to cache for future runs...\")\n",
    "        save_preprocessed_data(augmented_images, augmented_labels, OUTPUT_PATH, 'preprocessed_uniform.npz')\n",
    "        print(\"âœ“ Preprocessed data saved to cache!\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULTS (UNIFORM 30%):\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Dataset shape: {augmented_images.shape}\")\n",
    "    print(f\"Labels shape: {augmented_labels.shape}\")\n",
    "    print(f\"Total images: {len(augmented_images)}\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š Uniform class distribution:\")\n",
    "    unique, counts = np.unique(augmented_labels, return_counts=True)\n",
    "    class_names_dict = {0: 'glass', 1: 'paper', 2: 'cardboard', 3: 'plastic', 4: 'metal', 5: 'trash'}\n",
    "    total = len(augmented_images)\n",
    "    for class_id, count in zip(unique, counts):\n",
    "        pct = (count / total * 100)\n",
    "        print(f\"  {class_names_dict[class_id]}: {count} images ({pct:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nâœ… Uniform augmentation (30% per class):\")\n",
    "    print(\"   â€¢ Each class increased by 30%\")\n",
    "    print(\"   â€¢ Maintains original distribution\")\n",
    "    print(\"   â€¢ No risk of overfitting to augmented minority samples\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ“ Data ready for feature extraction!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f284f",
   "metadata": {},
   "source": [
    "## 2. Test Feature Extraction Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aaad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import rand\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST IMAGE & FEATURE EXTRACTION DEMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define CLASS_NAMES dictionary for reference\n",
    "CLASS_NAMES = {\n",
    "    0: 'glass', \n",
    "    1: 'paper', \n",
    "    2: 'cardboard', \n",
    "    3: 'plastic', \n",
    "    4: 'metal', \n",
    "    5: 'trash'\n",
    "}\n",
    "\n",
    "# Get a test image\n",
    "test_idx = int(rand() * len(augmented_images))\n",
    "test_image = augmented_images[test_idx]\n",
    "test_label = augmented_labels[test_idx]\n",
    "\n",
    "print(f\"\\nðŸ“· Test Image:\")\n",
    "print(f\"  - Shape: {test_image.shape}\")\n",
    "print(f\"  - Class: {CLASS_NAMES[test_label]}\")\n",
    "print(f\"  - Data type: {test_image.dtype}\")\n",
    "\n",
    "# Display the test image\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "test_image_rgb = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "ax.imshow(test_image_rgb)\n",
    "ax.set_title(f'Test Image: {CLASS_NAMES[test_label].upper()}', fontsize=14, fontweight='bold')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Import ENHANCED material-focused feature extraction\n",
    "from feature_extraction import (\n",
    "    extract_color_histogram, \n",
    "    extract_lbp_features,\n",
    "    extract_shape_features,\n",
    "    extract_edge_features,\n",
    "    extract_combined_features\n",
    ")\n",
    "\n",
    "# Extract each feature type separately\n",
    "color_feat = extract_color_histogram(test_image, bins=32)\n",
    "lbp_feat = extract_lbp_features(test_image)\n",
    "shape_feat = extract_shape_features(test_image)\n",
    "edge_feat = extract_edge_features(test_image)\n",
    "combined_feat = extract_combined_features(test_image)\n",
    "\n",
    "print(f\"\\nðŸŽ¨ ENHANCED Features for Glass/Metal/Plastic:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  âœ“ Color + Brightness/Reflect: {len(color_feat):>3} features\")\n",
    "print(f\"      â€¢ HSV histogram (96)\")\n",
    "print(f\"      â€¢ Color moments (9)\")\n",
    "print(f\"      â€¢ Brightness/reflectivity (13) - NEW!\")\n",
    "print(f\"      â†’ Glass: bright, low saturation, specular\")\n",
    "print(f\"      â†’ Metal: shiny, variable brightness\")\n",
    "print(f\"      â†’ Plastic: colored, uniform brightness\")\n",
    "print(f\"\")\n",
    "print(f\"  âœ“ LBP (4-scale):              {len(lbp_feat):>3} features\")\n",
    "print(f\"      â€¢ Radii: [1, 2, 5, 8] - ENHANCED!\")\n",
    "print(f\"      â€¢ Points: [8, 16, 24, 24]\")\n",
    "print(f\"      â†’ Glass: very smooth\")\n",
    "print(f\"      â†’ Plastic: semi-smooth + molding\")\n",
    "print(f\"      â†’ Metal: grain + scratches\")\n",
    "print(f\"\")\n",
    "print(f\"  âœ“ Edge Statistics:            {len(edge_feat):>3} features\")\n",
    "print(f\"  âœ“ Shape (Hu Moments):         {len(shape_feat):>3} features\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  ðŸ“¦ Combined (Normalized):     {len(combined_feat):>3} features\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Visualize feature distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('ENHANCED Features: Brightness/Reflectivity + 4-Scale Texture', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot each feature type\n",
    "axes[0, 0].hist(color_feat, bins=50, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Color + Brightness/Reflectivity (~118)', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Feature Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "axes[0, 1].hist(lbp_feat, bins=50, color='#2ecc71', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('4-Scale LBP Texture (~68)', fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Feature Value')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "axes[1, 0].hist(edge_feat, bins=20, color='#e67e22', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Edge Features (6)', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Feature Value')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1, 1].hist(combined_feat, bins=50, color='#34495e', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Combined Normalized (~199)', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_xlabel('Normalized Value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6843f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function for batch feature extraction\n",
    "from feature_extraction import extract_features_from_dataset\n",
    "\n",
    "# Extract features from entire augmented dataset using CNN\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTING FEATURES FROM ENTIRE DATASET - CNN METHOD\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total images to process: {len(augmented_images)}\")\n",
    "print(f\"Using: MobileNetV2 (pre-trained on ImageNet)\")\n",
    "print(f\"Expected features: 1280 per image\")\n",
    "\n",
    "all_features = extract_features_from_dataset(augmented_images, method='cnn')\n",
    "\n",
    "print(f\"\\nFeature extraction complete!\")\n",
    "print(f\"Final feature matrix shape: {all_features.shape}\")\n",
    "print(f\"  - Samples: {all_features.shape[0]}\")\n",
    "print(f\"  - Features per sample: {all_features.shape[1]}\")\n",
    "print(f\"\\nâœ“ Ready for classifier training with CNN features!\")\n",
    "print(f\"âœ“ Expected accuracy improvement: 5-10% over handcrafted features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aedde5",
   "metadata": {},
   "source": [
    "## 3. Feature Scaling (Critical for SVM and k-NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b438b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "X = all_features\n",
    "y = augmented_labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply StandardScaler (CRITICAL for SVM and k-NN)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Get actual class names (only classes present in data)\n",
    "unique_classes = np.unique(y)\n",
    "actual_class_names = [CLASS_NAMES[i] for i in unique_classes]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training set: {X_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_test_scaled.shape}\")\n",
    "print(f\"\\nClasses in dataset: {len(unique_classes)}\")\n",
    "print(f\"Class names: {actual_class_names}\")\n",
    "print(f\"\\nFeature scaling applied (StandardScaler)\")\n",
    "print(f\"  - Training features: mean=0, std=1\")\n",
    "print(f\"  - Test features: scaled using training statistics\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0cd862",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"K-NN CLASSIFIER (with Unknown Rejection)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train k-NN\n",
    "start_time = time.time()\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Get prediction probabilities for confidence-based rejection\n",
    "y_proba_knn = knn.predict_proba(X_test_scaled)\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "# Implement Unknown rejection: if max probability < threshold, classify as \"unknown\" (ID 6)\n",
    "confidence_threshold = 0.5  # Adjust this threshold based on performance\n",
    "max_proba = np.max(y_proba_knn, axis=1)\n",
    "\n",
    "# Create predictions with unknown class\n",
    "y_pred_knn_with_unknown = y_pred_knn.copy()\n",
    "y_pred_knn_with_unknown[max_proba < confidence_threshold] = 6  # ID 6 = unknown\n",
    "\n",
    "# Calculate accuracies\n",
    "knn_accuracy = knn.score(X_test_scaled, y_test) * 100\n",
    "num_rejected = np.sum(max_proba < confidence_threshold)\n",
    "rejection_rate = (num_rejected / len(y_test)) * 100\n",
    "\n",
    "print(f\"\\nâœ“ Training completed in {train_time:.2f} seconds\")\n",
    "print(f\"âœ“ Overall Accuracy (without rejection): {knn_accuracy:.2f}%\")\n",
    "print(f\"âœ“ Rejection rate: {rejection_rate:.2f}% ({num_rejected}/{len(y_test)} samples)\")\n",
    "print(f\"âœ“ Confidence threshold: {confidence_threshold}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT (without rejection):\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred_knn, target_names=actual_class_names, zero_division=0))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(\"=\"*60)\n",
    "cm = confusion_matrix(y_test, y_pred_knn)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=actual_class_names,\n",
    "            yticklabels=actual_class_names)\n",
    "plt.title(f'k-NN Confusion Matrix (Acc: {knn_accuracy:.2f}%)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show confidence distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(max_proba, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=confidence_threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {confidence_threshold}')\n",
    "plt.xlabel('Maximum Probability (Confidence)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('k-NN Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Confidence Analysis:\")\n",
    "print(f\"  - Mean confidence: {max_proba.mean():.4f}\")\n",
    "print(f\"  - Median confidence: {np.median(max_proba):.4f}\")\n",
    "print(f\"  - Min confidence: {max_proba.min():.4f}\")\n",
    "print(f\"  - Max confidence: {max_proba.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e7cbf",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf434d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import time\n",
    "print(\"=\"*60)\n",
    "print(\"SVM CLASSIFIER (with Unknown Rejection)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train SVM with probability=True for confidence scores\n",
    "start_time = time.time()\n",
    "svm = SVC(kernel='rbf', C=10, gamma='scale', probability=True, random_state=42)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Get prediction probabilities for confidence-based rejection\n",
    "y_proba_svm = svm.predict_proba(X_test_scaled)\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "\n",
    "# Implement Unknown rejection: if max probability < threshold, classify as \"unknown\" (ID 6)\n",
    "confidence_threshold = 0.5  # Adjust this threshold based on performance\n",
    "max_proba_svm = np.max(y_proba_svm, axis=1)\n",
    "\n",
    "# Create predictions with unknown class\n",
    "y_pred_svm_with_unknown = y_pred_svm.copy()\n",
    "y_pred_svm_with_unknown[max_proba_svm < confidence_threshold] = 6  # ID 6 = unknown\n",
    "\n",
    "# Calculate accuracies\n",
    "svm_accuracy = svm.score(X_test_scaled, y_test) * 100\n",
    "num_rejected_svm = np.sum(max_proba_svm < confidence_threshold)\n",
    "rejection_rate_svm = (num_rejected_svm / len(y_test)) * 100\n",
    "\n",
    "print(f\"\\nâœ“ Training completed in {train_time:.2f} seconds\")\n",
    "print(f\"âœ“ Overall Accuracy (without rejection): {svm_accuracy:.2f}%\")\n",
    "print(f\"âœ“ Rejection rate: {rejection_rate_svm:.2f}% ({num_rejected_svm}/{len(y_test)} samples)\")\n",
    "print(f\"âœ“ Confidence threshold: {confidence_threshold}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT (without rejection):\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred_svm, target_names=actual_class_names, zero_division=0))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(\"=\"*60)\n",
    "cm = confusion_matrix(y_test, y_pred_svm)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=actual_class_names,\n",
    "            yticklabels=actual_class_names)\n",
    "plt.title(f'SVM Confusion Matrix (Acc: {svm_accuracy:.2f}%)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show confidence distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(max_proba_svm, bins=50, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=confidence_threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {confidence_threshold}')\n",
    "plt.xlabel('Maximum Probability (Confidence)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('SVM Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Confidence Analysis:\")\n",
    "print(f\"  - Mean confidence: {max_proba_svm.mean():.4f}\")\n",
    "print(f\"  - Median confidence: {np.median(max_proba_svm):.4f}\")\n",
    "print(f\"  - Min confidence: {max_proba_svm.min():.4f}\")\n",
    "print(f\"  - Max confidence: {max_proba_svm.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f0af2d",
   "metadata": {},
   "source": [
    "## Save The Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bee3d32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING AND SAVING ALL MODELS\n",
      "============================================================\n",
      "âœ“ Class names saved\n",
      "\n",
      "------------------------------------------------------------\n",
      "Processing CNN Features...\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "EXTRACTING FEATURES: CNN METHOD\n",
      "============================================================\n",
      "Total images: 2421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:   0%|          | 0/2421 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MobileNetV2 model (first time only)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\COURSES\\ML\\Project\\notebooks\\../src\\feature_extraction.py:418: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  extract_cnn_features.mobilenet_model = MobileNetV2(\n",
      "Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2421/2421 [03:07<00:00, 12.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction complete!\n",
      "Feature matrix shape: (2421, 1280)\n",
      "Features per image: 1280\n",
      "============================================================\n",
      "\n",
      "CNN features shape: (2421, 1280)\n",
      "âœ“ CNN scaler saved\n",
      "\n",
      "Training SVM with CNN features...\n",
      "âœ“ SVM+CNN saved (Accuracy: 91.34%)\n",
      "\n",
      "Training KNN with CNN features...\n",
      "âœ“ KNN+CNN saved (Accuracy: 83.71%)\n",
      "\n",
      "------------------------------------------------------------\n",
      "Processing Handcrafted Features...\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "EXTRACTING FEATURES: COMBINED METHOD\n",
      "============================================================\n",
      "Total images: 2421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2421/2421 [02:12<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction complete!\n",
      "Feature matrix shape: (2421, 211)\n",
      "Features per image: 211\n",
      "============================================================\n",
      "\n",
      "Handcrafted features shape: (2421, 211)\n",
      "âœ“ Handcrafted scaler saved\n",
      "\n",
      "Training SVM with handcrafted features...\n",
      "âœ“ SVM+Handcrafted saved (Accuracy: 80.62%)\n",
      "\n",
      "Training KNN with handcrafted features...\n",
      "âœ“ KNN+Handcrafted saved (Accuracy: 64.95%)\n",
      "\n",
      "============================================================\n",
      "MODEL SUMMARY\n",
      "============================================================\n",
      "\n",
      "Model                       Accuracy\n",
      "-----------------------------------\n",
      "SVM + CNN                     91.34%\n",
      "KNN + CNN                     83.71%\n",
      "SVM + Handcrafted             80.62%\n",
      "KNN + Handcrafted             64.95%\n",
      "\n",
      "âœ… All models saved to: f:\\COURSES\\ML\\Project\\notebooks\\..\\models\n",
      "\n",
      "Saved files:\n",
      "  - class_names.pkl\n",
      "  - feature_scaler.pkl\n",
      "  - feature_scaler_cnn.pkl\n",
      "  - feature_scaler_handcrafted.pkl\n",
      "  - knn_classifier_cnn.pkl\n",
      "  - knn_classifier_handcrafted.pkl\n",
      "  - svm_classifier_cnn.pkl\n",
      "  - svm_classifier_handcrafted.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from pathlib import Path\n",
    "from data_preprocessing import CLASS_NAMES\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "from feature_extraction import extract_features_from_dataset\n",
    "\n",
    "MODEL_DIR = Path(\"../models\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING AND SAVING ALL MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save class names (shared by all models)\n",
    "joblib.dump(CLASS_NAMES, MODEL_DIR / \"class_names.pkl\")\n",
    "print(\"âœ“ Class names saved\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 1. CNN FEATURES\n",
    "# ============================================================\n",
    "print(\"-\"*60)\n",
    "print(\"Processing CNN Features...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Extract CNN features\n",
    "cnn_features = extract_features_from_dataset(augmented_images, method='cnn')\n",
    "print(f\"CNN features shape: {cnn_features.shape}\")\n",
    "\n",
    "# Split data\n",
    "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(\n",
    "    cnn_features, augmented_labels, test_size=0.2, random_state=42, stratify=augmented_labels\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_cnn = StandardScaler()\n",
    "X_train_cnn_scaled = scaler_cnn.fit_transform(X_train_cnn)\n",
    "X_test_cnn_scaled = scaler_cnn.transform(X_test_cnn)\n",
    "\n",
    "# Save CNN scaler\n",
    "joblib.dump(scaler_cnn, MODEL_DIR / \"feature_scaler_cnn.pkl\")\n",
    "print(\"âœ“ CNN scaler saved\")\n",
    "\n",
    "# Train and save SVM with CNN\n",
    "print(\"\\nTraining SVM with CNN features...\")\n",
    "svm_cnn = SVC(kernel='rbf', C=10, gamma='scale', probability=True, random_state=42)\n",
    "svm_cnn.fit(X_train_cnn_scaled, y_train_cnn)\n",
    "svm_cnn_acc = svm_cnn.score(X_test_cnn_scaled, y_test_cnn)\n",
    "joblib.dump(svm_cnn, MODEL_DIR / \"svm_classifier_cnn.pkl\")\n",
    "print(f\"âœ“ SVM+CNN saved (Accuracy: {svm_cnn_acc*100:.2f}%)\")\n",
    "\n",
    "# Train and save KNN with CNN\n",
    "print(\"\\nTraining KNN with CNN features...\")\n",
    "knn_cnn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_cnn.fit(X_train_cnn_scaled, y_train_cnn)\n",
    "knn_cnn_acc = knn_cnn.score(X_test_cnn_scaled, y_test_cnn)\n",
    "joblib.dump(knn_cnn, MODEL_DIR / \"knn_classifier_cnn.pkl\")\n",
    "print(f\"âœ“ KNN+CNN saved (Accuracy: {knn_cnn_acc*100:.2f}%)\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. HANDCRAFTED FEATURES\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Processing Handcrafted Features...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Extract handcrafted features\n",
    "handcrafted_features = extract_features_from_dataset(augmented_images, method='combined')\n",
    "print(f\"Handcrafted features shape: {handcrafted_features.shape}\")\n",
    "\n",
    "# Split data\n",
    "X_train_hc, X_test_hc, y_train_hc, y_test_hc = train_test_split(\n",
    "    handcrafted_features, augmented_labels, test_size=0.2, random_state=42, stratify=augmented_labels\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler_hc = StandardScaler()\n",
    "X_train_hc_scaled = scaler_hc.fit_transform(X_train_hc)\n",
    "X_test_hc_scaled = scaler_hc.transform(X_test_hc)\n",
    "\n",
    "# Save handcrafted scaler\n",
    "joblib.dump(scaler_hc, MODEL_DIR / \"feature_scaler_handcrafted.pkl\")\n",
    "print(\"âœ“ Handcrafted scaler saved\")\n",
    "\n",
    "# Train and save SVM with handcrafted\n",
    "print(\"\\nTraining SVM with handcrafted features...\")\n",
    "svm_hc = SVC(kernel='rbf', C=10, gamma='scale', probability=True, random_state=42)\n",
    "svm_hc.fit(X_train_hc_scaled, y_train_hc)\n",
    "svm_hc_acc = svm_hc.score(X_test_hc_scaled, y_test_hc)\n",
    "joblib.dump(svm_hc, MODEL_DIR / \"svm_classifier_handcrafted.pkl\")\n",
    "print(f\"âœ“ SVM+Handcrafted saved (Accuracy: {svm_hc_acc*100:.2f}%)\")\n",
    "\n",
    "# Train and save KNN with handcrafted\n",
    "print(\"\\nTraining KNN with handcrafted features...\")\n",
    "knn_hc = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_hc.fit(X_train_hc_scaled, y_train_hc)\n",
    "knn_hc_acc = knn_hc.score(X_test_hc_scaled, y_test_hc)\n",
    "joblib.dump(knn_hc, MODEL_DIR / \"knn_classifier_handcrafted.pkl\")\n",
    "print(f\"âœ“ KNN+Handcrafted saved (Accuracy: {knn_hc_acc*100:.2f}%)\")\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Model':<25} {'Accuracy':>10}\")\n",
    "print(\"-\"*35)\n",
    "print(f\"{'SVM + CNN':<25} {svm_cnn_acc*100:>9.2f}%\")\n",
    "print(f\"{'KNN + CNN':<25} {knn_cnn_acc*100:>9.2f}%\")\n",
    "print(f\"{'SVM + Handcrafted':<25} {svm_hc_acc*100:>9.2f}%\")\n",
    "print(f\"{'KNN + Handcrafted':<25} {knn_hc_acc*100:>9.2f}%\")\n",
    "\n",
    "print(f\"\\nâœ… All models saved to: {MODEL_DIR.absolute()}\")\n",
    "print(\"\\nSaved files:\")\n",
    "for f in sorted(MODEL_DIR.glob(\"*.pkl\")):\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918a0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
